{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPg/9pRPXrLgOvmqFp75OAp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aemrhb/IPI_project-seminar/blob/main/finetune_multi_files.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Aqr_cPX311e",
        "outputId": "499eca09-4698-411a-993c-0d7dc52a8912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/My Drive/faster-Rccn-finetune"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqjBVmL64A7B",
        "outputId": "93a11ddd-83d8-4510-84c1-7ac746deea9c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/faster-Rccn-finetune\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download TorchVision repo to use some files from\n",
        "# references/detection\n",
        "!pip install pycocotools --quiet\n",
        "# !git clone https://github.com/pytorch/vision.git\n",
        "!git checkout v0.3.0\n",
        "\n",
        "!cp vision/references/detection/utils.py ./\n",
        "!cp vision/references/detection/transforms.py ./\n",
        "!cp vision/references/detection/coco_eval.py ./\n",
        "!cp vision/references/detection/engine.py ./\n",
        "!cp vision/references/detection/coco_utils.py ./"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZelB9h6H6PnJ",
        "outputId": "b1f15e05-bab7-4051-981c-5ff8d7b76033"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any parent up to mount point /content)\n",
            "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "# from torchvision.datasets import YourCustomDataset\n",
        "from torch.optim import SGD\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as T\n",
        "# Basic python and ML Libraries\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# for ignoring warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# We will be reading images using OpenCV\n",
        "import cv2\n",
        "\n",
        "# xml library for parsing xml files\n",
        "from xml.etree import ElementTree as et\n",
        "\n",
        "# matplotlib for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# torchvision libraries\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as torchtrans\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# these are the helper libraries imported.\n",
        "# from engine import train_one_epoch, evaluate\n",
        "# import utils\n",
        "# import transforms as T\n",
        "\n",
        "# for image augmentations\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "# matplotlib for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "files_dirs = [\n",
        "    '/content/drive/My Drive/faster-Rccn-finetune/data/Kuken_videos-00_0',\n",
        "    '/content/drive/My Drive/faster-Rccn-finetune/data/Kuken_videos-30_0',\n",
        "    '/content/drive/My Drive/faster-Rccn-finetune/data/Kuken_videos-60'\n",
        "]\n",
        "\n",
        "class CombinedDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, files_dirs, width, height, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.files_dirs = files_dirs\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.imgs = []\n",
        "\n",
        "        for files_dir in files_dirs:\n",
        "            imge_path = os.path.join(files_dir, \"img1\")\n",
        "            self.imgs.extend(list(sorted(os.listdir(imge_path))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        files_dir_idx = idx // len(self.imgs)\n",
        "        img_idx = idx % len(self.imgs)\n",
        "\n",
        "        files_dir = self.files_dirs[files_dir_idx]\n",
        "        imge_path = os.path.join(files_dir, \"img1\")\n",
        "        img_name = self.imgs[img_idx]\n",
        "        img_path = os.path.join(imge_path, img_name)\n",
        "\n",
        "        img = cv2.imread(img_path)\n",
        "        wt = img.shape[1]\n",
        "        ht = img.shape[0]\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n",
        "\n",
        "        img_res /= 255.0\n",
        "\n",
        "\n",
        "        annots_path  = os.path.join(files_dir ,\"annot\")\n",
        "        annot_file = img_name[:-4] + '.txt'\n",
        "        annot_path = os.path.join(annots_path,annot_file)\n",
        "        with open(annot_path, \"r\") as file:\n",
        "          file_contents = file.read()\n",
        "        lines = file_contents.split(\"\\n\")\n",
        "        data = np.matrix(lines)\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        area = []\n",
        "\n",
        "        for i in range(data.shape[1] - 1 ):\n",
        "          boxe = []\n",
        "          line = data[0,i].split(\",\")\n",
        "          # print(line)\n",
        "          strr = line[2:6]\n",
        "\n",
        "          res = [float(t) for t in strr]\n",
        "\n",
        "          xmin_corr = (res[0]/wt)*self.width\n",
        "          xmax_corr = (res[0]/wt)*self.width + (res[2]/wt)*self.width\n",
        "          ymin_corr = (res[1]/ht)*self.height\n",
        "          ymax_corr = (res[1]/ht)*self.height + (res[3]/ht)*self.height\n",
        "\n",
        "          are = int((res[2] - res[0]) * (res[3] - res[1]))\n",
        "          area.append(are)\n",
        "          boxes.append([xmin_corr, ymin_corr, xmax_corr, ymax_corr])\n",
        "          labels.append(float(line[7]))\n",
        "\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        area = torch.as_tensor(area, dtype=torch.int64)\n",
        "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
        "        image_id = torch.tensor([idx])\n",
        "\n",
        "\n",
        "        target = {}\n",
        "\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "        target[\"image_id\"] = image_id\n",
        "\n",
        "        if self.transforms:\n",
        "\n",
        "            sample = self.transforms(image = img_res)\n",
        "                                    #  bboxes = target['boxes'],\n",
        "                                    #  labels = labels)\n",
        "\n",
        "            img_res = sample['image']\n",
        "            # target['boxes'] = torch.Tensor(sample['bboxes'])\n",
        "\n",
        "        return img_res, target\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the size of the dataset\n",
        "        return len(self.imgs)\n",
        "\n",
        "\n",
        "\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T\n",
        "\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    # converts the image, a PIL image, into a PyTorch Tensor\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        # during training, randomly flip the training images\n",
        "        # and ground-truth for data augmentation\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)\n",
        "\n",
        "def get_transform(train):\n",
        "    if train:\n",
        "        return A.Compose([\n",
        "            ToTensorV2(p=1.0)\n",
        "        ])\n",
        "    else:\n",
        "        return A.Compose([\n",
        "            ToTensorV2(p=1.0)\n",
        "        ])\n",
        "\n",
        "dataset = CombinedDataset(files_dirs, 224, 224)\n",
        "# dataset[0]\n",
        "# print('length of dataset = ', len(dataset), '\\n')\n",
        "\n",
        "# getting the image and target for a test index.  Feel free to change the index.\n",
        "img, target = dataset[87]\n",
        "# for key, value in target.items():\n",
        "#     print(key, value)\n",
        "print(img.shape, '\\n',target)\n",
        "\n",
        "def plot_img_bbox(img, target):\n",
        "    # plot the image and bboxes\n",
        "    # Bounding boxes are defined as follows: x-min y-min width height\n",
        "    fig, a = plt.subplots(1,1)\n",
        "    fig.set_size_inches(5,5)\n",
        "    a.imshow(img)\n",
        "    for box in (target['boxes']):\n",
        "        x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n",
        "        rect = patches.Rectangle((x, y),\n",
        "                                 width, height,\n",
        "                                 linewidth = 2,\n",
        "                                 edgecolor = 'r',\n",
        "                                 facecolor = 'none')\n",
        "\n",
        "        # Draw the bounding box on top of the image\n",
        "        a.add_patch(rect)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# plotting the image with bboxes. Feel free to change the index\n",
        "img, target = dataset[68]\n",
        "plot_img_bbox(img, target)\n",
        "\n",
        "\n",
        "def get_object_detection_model(num_classes):\n",
        "\n",
        "    # load a model pre-trained pre-trained on COCO\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "dataset = CombinedDataset(files_dir, 480, 480, transforms= get_transform(train=True))\n",
        "dataset_test = CombinedDataset(files_dir, 480, 480, transforms= get_transform(train=False))\n",
        "\n",
        "# split the dataset in train and test set\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "\n",
        "# train test split\n",
        "test_split = 0.2\n",
        "tsize = int(len(dataset)*test_split)\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:-tsize])\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[-tsize:])\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=10, shuffle=True, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=10, shuffle=False, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "\n",
        "dataset = CombinedDataset(files_dir, 480, 480, transforms= get_transform(train=True))\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
        "    collate_fn=utils.collate_fn\n",
        ")\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "num_classes = 3\n",
        "print(device)\n",
        "model = get_object_detection_model(num_classes)\n",
        "model.to(device)\n",
        "\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# and a learning rate scheduler which decreases the learning rate by\n",
        "# 10x every 3 epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)\n",
        "\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "  lr_scheduler.step()\n",
        "\n",
        "  evaluate(model, data_loader_test, device=device)\n",
        "\n",
        "\n",
        "  num_epochs = 3\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "  lr_scheduler.step()\n",
        "\n",
        "  # evaluate(model, data_loader_test, device=device)\n",
        "\n",
        "\n",
        "# pick one image from the test set\n",
        "img, target = dataset_test[10]\n",
        "# put the model in evaluation mode\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    prediction = model([img.to(device)])[0]\n",
        "\n",
        "print('predicted #boxes: ', len(prediction['labels']))\n",
        "print('real #boxes: ', len(target['labels']))\n",
        "\n",
        "\n",
        "def torch_to_pil(img):\n",
        "    return torchtrans.ToPILImage()(img).convert('RGB')\n",
        "\n",
        "\n",
        "print('EXPECTED OUTPUT')\n",
        "plot_img_bbox(torch_to_pil(img), target)\n",
        "\n",
        "\n",
        "\n",
        "prediction = {key: tensor.cpu() for key, tensor in prediction.items()}\n",
        "print('MODEL OUTPUT')\n",
        "plot_img_bbox(torch_to_pil(img), prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "eThSBElA6Rsf",
        "outputId": "253085b8-59a9-49a6-bd8b-8e4907528713"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-db8aa2b52f2e>\u001b[0m in \u001b[0;36m<cell line: 178>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    176\u001b[0m         ])\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCombinedDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_dirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;31m# dataset[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;31m# print('length of dataset = ', len(dataset), '\\n')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-db8aa2b52f2e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, files_dirs, width, height, transforms)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfiles_dir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles_dirs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mimge_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"img1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimge_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/faster-Rccn-finetune/data/Kuken_videos-00_0/img1'"
          ]
        }
      ]
    }
  ]
}